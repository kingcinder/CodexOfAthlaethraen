"""Utilities for managing the scalable AI memory containment vessel.

This module implements a light-weight persistence layer backed by SQLite to keep
track of memory fragments generated by The Assistant.  The goal is to provide a
single drop-in module that mirrors the functionality of the standalone
"scalable-ai-memory-containment-vessel-script" project while integrating with
this codebase.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Iterable, Iterator, Mapping, MutableMapping, Sequence
import json
import logging
import sqlite3
import textwrap

logger = logging.getLogger(__name__)


@dataclass(slots=True)
class MemoryFragment:
    """Represents a single unit of memory to be stored in the vessel."""

    content: str
    importance: float = 0.5
    metadata: MutableMapping[str, object] = field(default_factory=dict)
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

    def as_record(self) -> Mapping[str, object]:
        """Return a mapping suitable for storage in SQLite."""

        importance = min(max(self.importance, 0.0), 1.0)
        return {
            "content": self.content.strip(),
            "importance": importance,
            "created_at": self.created_at.astimezone(timezone.utc).isoformat(),
            "metadata": json.dumps(self.metadata, ensure_ascii=False),
        }


class MemoryContainmentVessel:
    """Persistent storage for assistant memory fragments.

    The vessel is intentionally thin: it stores fragments in SQLite, creates a
    set of indexes to keep things fast, and exposes query helpers for the
    higher-level application code.
    """

    def __init__(self, storage_path: Path | str):
        self.storage_path = Path(storage_path)
        self.storage_path.parent.mkdir(parents=True, exist_ok=True)
        self._ensure_schema()

    # ------------------------------------------------------------------
    # SQLite helpers
    def _connect(self) -> sqlite3.Connection:
        conn = sqlite3.connect(self.storage_path)
        conn.row_factory = sqlite3.Row
        return conn

    def _ensure_schema(self) -> None:
        with self._connect() as conn:
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS memories (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    content TEXT NOT NULL,
                    created_at TEXT NOT NULL,
                    importance REAL NOT NULL,
                    tags TEXT NOT NULL DEFAULT '',
                    metadata TEXT NOT NULL DEFAULT '{}'
                )
                """
            )
            conn.execute(
                """
                CREATE INDEX IF NOT EXISTS idx_memories_created
                ON memories(created_at DESC)
                """
            )
            conn.execute(
                """
                CREATE INDEX IF NOT EXISTS idx_memories_importance
                ON memories(importance DESC)
                """
            )
            conn.execute(
                """
                CREATE INDEX IF NOT EXISTS idx_memories_tags
                ON memories(tags)
                """
            )
            conn.commit()

    # ------------------------------------------------------------------
    # Memory ingestion
    def store(
        self, fragment: MemoryFragment, *, tags: Sequence[str] | None = None
    ) -> int:
        """Store a single fragment and return the SQLite row id."""

        record = fragment.as_record()
        record["tags"] = self._tags_to_text(tags)
        with self._connect() as conn:
            cursor = conn.execute(
                """
                INSERT INTO memories(content, created_at, importance, tags, metadata)
                VALUES (:content, :created_at, :importance, :tags, :metadata)
                """,
                record,
            )
            conn.commit()
            row_id = cursor.lastrowid
        logger.debug("Stored memory %s", row_id)
        return int(row_id)

    def store_many(
        self, fragments: Iterable[MemoryFragment], *, tags: Sequence[str] | None = None
    ) -> list[int]:
        """Store a batch of fragments returning their row ids."""

        tag_text = self._tags_to_text(tags)
        rows = [
            (
                fragment.content.strip(),
                fragment.created_at.astimezone(timezone.utc).isoformat(),
                min(max(fragment.importance, 0.0), 1.0),
                tag_text,
                json.dumps(fragment.metadata, ensure_ascii=False),
            )
            for fragment in fragments
        ]
        if not rows:
            return []

        with self._connect() as conn:
            cursor = conn.executemany(
                """
                INSERT INTO memories(content, created_at, importance, tags, metadata)
                VALUES (?, ?, ?, ?, ?)
                """,
                rows,
            )
            conn.commit()
            start_id = cursor.lastrowid or 0
            return list(range(start_id - len(rows) + 1, start_id + 1))

    # ------------------------------------------------------------------
    # Query helpers
    def iter_recent(
        self, *, limit: int = 20, tags: Sequence[str] | None = None
    ) -> Iterator[sqlite3.Row]:
        """Yield the most recent memories respecting optional tags."""

        tag_filter, params = self._tag_filter(tags)
        sql = f"""
            SELECT * FROM memories
            {tag_filter}
            ORDER BY datetime(created_at) DESC
            LIMIT :limit
        """
        params["limit"] = limit
        with self._connect() as conn:
            yield from conn.execute(sql, params)

    def search(
        self,
        query: str,
        *,
        limit: int = 20,
        tags: Sequence[str] | None = None,
        min_importance: float | None = None,
    ) -> list[sqlite3.Row]:
        """Return memories that fuzzily match the query string."""

        tag_filter, params = self._tag_filter(tags)
        where_clauses = [c for c in [tag_filter.strip()] if c]
        params.setdefault("pattern", f"%{query}%")
        where_clauses.append("content LIKE :pattern")
        if min_importance is not None:
            where_clauses.append("importance >= :min_importance")
            params["min_importance"] = float(min_importance)
        where_sql = "WHERE " + " AND ".join(where_clauses) if where_clauses else ""
        sql = f"""
            SELECT * FROM memories
            {where_sql}
            ORDER BY importance DESC, datetime(created_at) DESC
            LIMIT :limit
        """
        params["limit"] = limit
        with self._connect() as conn:
            return list(conn.execute(sql, params))

    def summarize(self, *, limit: int = 5) -> str:
        """Return a human readable summary of the most important memories."""

        with self._connect() as conn:
            rows = list(
                conn.execute(
                    """
                    SELECT content, importance, created_at
                    FROM memories
                    ORDER BY importance DESC, datetime(created_at) DESC
                    LIMIT ?
                    """,
                    (limit,),
                )
            )

        if not rows:
            return "No memories stored yet."

        summary_lines = ["Top priority memories:"]
        for idx, row in enumerate(rows, start=1):
            created = datetime.fromisoformat(row["created_at"]).astimezone(timezone.utc)
            bullet = textwrap.shorten(row["content"], width=120, placeholder="â€¦")
            summary_lines.append(
                f"{idx}. [{created.isoformat()} | importance {row['importance']:.2f}] {bullet}"
            )
        return "\n".join(summary_lines)

    def purge(
        self,
        *,
        before: datetime | None = None,
        older_than: timedelta | None = None,
        max_records: int | None = None,
        tags: Sequence[str] | None = None,
    ) -> int:
        """Delete memories using temporal or count-based policies."""

        if older_than is not None:
            before = datetime.now(timezone.utc) - older_than
        params: dict[str, object] = {}
        clauses: list[str] = []
        if before is not None:
            if before.tzinfo is None:
                before = before.replace(tzinfo=timezone.utc)
            params["before"] = before.astimezone(timezone.utc).isoformat()
            clauses.append("datetime(created_at) <= datetime(:before)")
        tag_clause, tag_params = self._tag_filter(tags)
        if tag_clause:
            clauses.append(tag_clause.replace("WHERE ", "", 1))
            params.update(tag_params)
        where_sql = "WHERE " + " AND ".join(clauses) if clauses else ""

        with self._connect() as conn:
            if max_records is not None:
                selection_sql = f"""
                    SELECT rowid FROM memories
                    {where_sql}
                    ORDER BY datetime(created_at) ASC
                    LIMIT :limit
                """
                selection_params = dict(params)
                selection_params["limit"] = max_records
                rowids = [
                    row["rowid"]
                    for row in conn.execute(selection_sql, selection_params)
                ]
                if not rowids:
                    return 0
                conn.executemany(
                    "DELETE FROM memories WHERE rowid = ?",
                    ((rowid,) for rowid in rowids),
                )
                conn.commit()
                return len(rowids)

            sql = f"DELETE FROM memories {where_sql}"
            cursor = conn.execute(sql, params)
            conn.commit()
            return cursor.rowcount

    def export(self, destination: Path | str, *, tags: Sequence[str] | None = None) -> Path:
        """Export memories to a newline-delimited JSON file."""

        destination_path = Path(destination)
        destination_path.parent.mkdir(parents=True, exist_ok=True)
        tag_filter, params = self._tag_filter(tags)
        sql = f"SELECT * FROM memories {tag_filter} ORDER BY datetime(created_at)"
        with self._connect() as conn, destination_path.open("w", encoding="utf-8") as handle:
            for row in conn.execute(sql, params):
                payload = {
                    "id": row["id"],
                    "content": row["content"],
                    "created_at": row["created_at"],
                    "importance": row["importance"],
                    "tags": row["tags"].split(",") if row["tags"] else [],
                    "metadata": json.loads(row["metadata"] or "{}"),
                }
                handle.write(json.dumps(payload, ensure_ascii=False) + "\n")
        logger.info("Exported memories to %s", destination_path)
        return destination_path

    # ------------------------------------------------------------------
    # Helpers
    @staticmethod
    def _tags_to_text(tags: Sequence[str] | None) -> str:
        if not tags:
            return ""
        return ",".join(sorted({tag.strip() for tag in tags if tag.strip()}))

    @staticmethod
    def _tag_filter(tags: Sequence[str] | None) -> tuple[str, dict[str, object]]:
        if not tags:
            return "", {}
        normalized = sorted({tag.strip() for tag in tags if tag.strip()})
        if not normalized:
            return "", {}
        like_clauses = [f"tags LIKE :tag{i}" for i, _ in enumerate(normalized)]
        params = {f"tag{i}": f"%{tag}%" for i, tag in enumerate(normalized)}
        clause = " WHERE " + " AND ".join(like_clauses)
        return clause, params


__all__ = ["MemoryContainmentVessel", "MemoryFragment"]
