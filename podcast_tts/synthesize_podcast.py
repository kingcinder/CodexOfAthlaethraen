#!/usr/bin/env python3
"""Synthesize a back-and-forth podcast style dialogue using Coqui TTS.

This script accepts a plain-text transcript without explicit speaker labels
and alternates between two (or more) configured voices to generate an audio
track for each turn as well as a combined mix.

Usage example::

    python synthesize_podcast.py transcript.txt \
        --output-dir build/output \
        --model tts_models/multilingual/multi-dataset/xtts_v2 \
        --language en \
        --speaker-wavs speaker_a.wav speaker_b.wav \
        --gap-ms 150

The script intentionally exposes simple heuristics for splitting dialogue
turns and assigning speakers. The defaults assume the transcript is already in
chronological order and each new paragraph indicates a speaker change. Advanced
logic can be enabled with ``--split-mode sentence`` to force sentence-level
splits and ``--turn-threshold`` to limit maximum consecutive turns per voice.
"""
from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Iterable, List, Sequence

from TTS.api import TTS


def _read_transcript(transcript_path: Path) -> str:
    text = transcript_path.read_text(encoding="utf-8")
    if not text.strip():
        raise ValueError("The transcript is empty. Please provide dialogue text.")
    return text


def _normalize_lines(text: str, split_mode: str) -> List[str]:
    """Split transcript into dialogue turns.

    Parameters
    ----------
    text:
        Raw transcript text.
    split_mode:
        Either ``paragraph`` (default) or ``sentence``.
    """
    import re

    # Normalise all Windows-style newlines and trim whitespace.
    normalized = text.replace("\r\n", "\n").strip()

    if split_mode == "sentence":
        parts = re.split(r"(?<=[.!?]\")\s+|(?<=[.!?])\s+(?=[\"'\(\[]*[A-Z0-9])", normalized)
    else:
        # Collapse multiple blank lines to a single delimiter and split.
        parts = re.split(r"\n\s*\n", normalized)

    turns = [re.sub(r"\s+", " ", part.strip()) for part in parts if part.strip()]
    if not turns:
        raise ValueError("No dialogue turns could be extracted. Check formatting.")
    return turns


def _cycle_speakers(speakers: Sequence[str], max_consecutive: int) -> Iterable[str]:
    if not speakers:
        raise ValueError("At least one speaker reference must be provided.")
    if max_consecutive < 1:
        raise ValueError("max_consecutive must be at least 1.")

    if len(speakers) == 1:
        while True:
            for _ in range(max_consecutive):
                yield speakers[0]
    else:
        while True:
            for speaker in speakers:
                for _ in range(max_consecutive):
                    yield speaker


def _load_speakers(args: argparse.Namespace) -> List[str]:
    """Resolve speaker identifiers from CLI arguments.

    The user can mix and match reference wav files, JSON lists of wavs, JSON
    profiles created via ``create_voice_profile.py`` and built-in speaker names.
    """

    resolved: List[str] = []

    if args.speaker_wavs:
        resolved.extend(str(Path(path).expanduser().resolve()) for path in args.speaker_wavs)

    if args.speaker_json:
        data = json.loads(Path(args.speaker_json).read_text(encoding="utf-8"))
        if not isinstance(data, list) or not all(isinstance(p, str) for p in data):
            raise ValueError("speaker_json must contain a JSON list of paths to wav files")
        resolved.extend(str(Path(path).expanduser().resolve()) for path in data)

    if args.speaker_profiles:
        for profile_path in args.speaker_profiles:
            profile_file = Path(profile_path).expanduser().resolve()
            profile_data = json.loads(profile_file.read_text(encoding="utf-8"))
            if not isinstance(profile_data, dict) or "reference_wav" not in profile_data:
                raise ValueError(
                    "Speaker profile JSON must contain a 'reference_wav' field generated by create_voice_profile.py"
                )
            resolved.append(str(Path(profile_data["reference_wav"]).expanduser().resolve()))

    if args.speakers:
        resolved.extend(args.speakers)

    if not resolved:
        raise ValueError(
            "Provide --speaker-wavs, --speaker-json, --speaker-profiles, or --speakers to select voices."
        )

    return resolved


def _prepare_output_dir(output_dir: Path) -> None:
    output_dir.mkdir(parents=True, exist_ok=True)


def _concatenate_turns(
    input_files: Sequence[Path],
    output_file: Path,
    gap_ms: float = 0.0,
    *,
    normalise: bool = False,
    peak: float = 0.98,
) -> None:
    import soundfile as sf
    import numpy as np

    sr: int | None = None
    channels: int | None = None

    segments: list[np.ndarray] = []
    for path in input_files:
        audio, current_sr = sf.read(path)
        if sr is None:
            sr = current_sr
        elif current_sr != sr:
            raise RuntimeError("All generated wav files must share the same sample rate.")

        if audio.ndim == 1:
            audio = audio[:, None]
        if channels is None:
            channels = audio.shape[1]
        elif audio.shape[1] != channels:
            raise RuntimeError("All generated wav files must have the same number of channels.")

        segments.append(audio.astype(np.float32, copy=False))

    if sr is None or channels is None:
        raise RuntimeError("No dialogue turns were generated; cannot create master mix.")

    gap_samples = 0
    if gap_ms > 0:
        gap_samples = max(1, int(round((gap_ms / 1000.0) * sr)))
    if gap_samples > 0:
        silence = np.zeros((gap_samples, channels), dtype=np.float32)
    else:
        silence = None

    timeline: list[np.ndarray] = []
    for index, segment in enumerate(segments):
        timeline.append(segment)
        if silence is not None and index < len(segments) - 1:
            timeline.append(silence)

    master = np.concatenate(timeline, axis=0)
    if normalise:
        max_abs = float(np.max(np.abs(master)))
        if max_abs > 0:
            master = (master / max_abs) * float(peak)
    sf.write(output_file, master, sr)


def synthesize_dialogue(args: argparse.Namespace) -> Path:
    transcript_path = Path(args.transcript).expanduser().resolve()
    output_dir = Path(args.output_dir).expanduser().resolve()
    _prepare_output_dir(output_dir)

    turns = _normalize_lines(_read_transcript(transcript_path), args.split_mode)
    speakers = _load_speakers(args)

    tts = TTS(model_name=args.model, progress_bar=args.progress).to(args.device)

    speaker_cycle = _cycle_speakers(speakers, args.turn_threshold)

    generated_paths: List[Path] = []
    for index, (turn, speaker_ref) in enumerate(zip(turns, speaker_cycle)):
        file_name = f"turn_{index + 1:03d}.wav"
        file_path = output_dir / file_name
        kwargs = {"file_path": str(file_path)}
        if speaker_ref.lower().endswith((".wav", ".mp3", ".flac", ".ogg")):
            kwargs["speaker_wav"] = speaker_ref
        else:
            kwargs["speaker"] = speaker_ref
        if args.language:
            kwargs["language"] = args.language
        if args.emotion:
            kwargs["emotion"] = args.emotion
        tts.tts_to_file(turn, **kwargs)
        generated_paths.append(file_path)

    mix_path = output_dir / "podcast_mix.wav"
    _concatenate_turns(
        generated_paths,
        mix_path,
        gap_ms=args.gap_ms,
        normalise=args.normalize_mix,
        peak=args.mix_peak,
    )
    return mix_path


def build_arg_parser() -> argparse.ArgumentParser:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument("transcript", help="Path to the input transcript text file.")
    parser.add_argument(
        "--output-dir",
        default="build/tts_output",
        help="Directory where synthesized wav files will be stored.",
    )
    parser.add_argument(
        "--model",
        default="tts_models/multilingual/multi-dataset/xtts_v2",
        help="Coqui TTS model identifier to load.",
    )
    parser.add_argument(
        "--language",
        default="en",
        help="Language token for multilingual models (e.g., en, es, de).",
    )
    parser.add_argument(
        "--speaker-wavs",
        nargs="*",
        help="Paths to reference audio samples for voice cloning (wav/flac/mp3).",
    )
    parser.add_argument(
        "--speaker-json",
        help="JSON file containing a list of reference audio sample paths.",
    )
    parser.add_argument(
        "--speaker-profiles",
        nargs="*",
        help="Voice profile JSON files produced by create_voice_profile.py.",
    )
    parser.add_argument(
        "--speakers",
        nargs="*",
        help="Names of built-in multi-speaker voices (if supported by the model).",
    )
    parser.add_argument(
        "--turn-threshold",
        type=int,
        default=1,
        help="Maximum consecutive turns before rotating to the next speaker.",
    )
    parser.add_argument(
        "--split-mode",
        choices=["paragraph", "sentence"],
        default="paragraph",
        help="Dialogue turn segmentation heuristic.",
    )
    parser.add_argument(
        "--emotion",
        help="Optional emotion/style token supported by certain models.",
    )
    parser.add_argument(
        "--device",
        default="auto",
        help="torch device to use (auto, cpu, cuda, or rocm).",
    )
    parser.add_argument(
        "--progress",
        action="store_true",
        help="Display the TTS library progress bar while generating audio.",
    )
    parser.add_argument(
        "--gap-ms",
        type=float,
        default=120.0,
        help="Silence inserted between turns in the combined mix (milliseconds).",
    )
    parser.add_argument(
        "--normalize-mix",
        action="store_true",
        help="Peak-normalise the combined podcast mix to avoid clipping.",
    )
    parser.add_argument(
        "--mix-peak",
        type=float,
        default=0.98,
        help="Target peak amplitude (0-1) when --normalize-mix is enabled.",
    )
    return parser


def main() -> None:
    parser = build_arg_parser()
    args = parser.parse_args()
    mix_path = synthesize_dialogue(args)
    print(f"Combined mix saved to: {mix_path}")


if __name__ == "__main__":
    main()
